{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q \"scikit-learn~=1.6\" \\\n",
    "    \"datasets==3.3.2\" \"huggingface-hub==0.29.1\" \\\n",
    "    \"transformers==4.49.0\" \"evaluate==0.4.3\" \\\n",
    "    \"peft==0.14.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f296737-6b42-49fd-b150-fb2ac7cf5e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dataset_name = \"SetFit/ag_news\"\n",
    "dataset_splits = [\"train\", \"test\"]\n",
    "model_name = \"openai-community/gpt2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7415d043-c071-471d-b87d-4789648926bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# the ag_news dataset is split into 120k train rows and 7.6k test rows.\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# reduce record cound to fit in memory (and run faster)\n",
    "dataset[\"train\"] = dataset[\"train\"].shuffle(seed=8).select(range(800))\n",
    "dataset[\"test\"] = dataset[\"test\"].shuffle(seed=8).select(range(200))\n",
    "\n",
    "# view the dataset characteristics\n",
    "print(\"train =\", dataset[\"train\"])\n",
    "print(\"train[0] =\", dataset[\"train\"][0])\n",
    "print(\"--------------------\")\n",
    "print(\"test =\", dataset[\"test\"])\n",
    "print(\"test[0] =\", dataset[\"test\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc93268f-28e8-454a-9fa3-2f66a8054d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# https://huggingface.co/docs/transformers/en/model_doc/gpt2#usage-tips\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "\n",
    "# \n",
    "tokenized_dataset = {}\n",
    "for split in dataset_splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(\n",
    "        lambda row: tokenizer(row[\"text\"], truncation=True), batched=True\n",
    "    )\n",
    "\n",
    "# inspect the special token ids\n",
    "print(f\"{tokenizer.eos_token_id=}\")\n",
    "print(f\"{tokenizer.pad_token_id=}\")\n",
    "\n",
    "# inspect the columns in the tokenized dataset\n",
    "print(f\"{tokenized_dataset['train']=}\")\n",
    "print(f\"{tokenized_dataset['test']=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=4,\n",
    "    id2label={ 0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\" },\n",
    "    label2id={ \"World\": 0, \"Sports\": 1, \"Business\": 2, \"Sci/Tech\": 3 },\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "# ensure all the parameters of the base model are frozen.\n",
    "# see: https://huggingface.co/transformers/v4.2.2/training.html\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/news_gpt2\",\n",
    "        # set the learning rate\n",
    "        learning_rate=0.005,\n",
    "        # Set the per device train batch size and eval batch size\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        # evaluate and save the model after each epoch\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf90bb-4058-40c7-9746-9355aa7c1a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are interested in the performance of the base model,\n",
    "# so will not train with this dataset.\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b9f55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c29fee2-875d-43fb-8f6a-38931a339f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# https://huggingface.co/docs/transformers/en/model_doc/gpt2#usage-tips\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    padding_side=\"left\"\n",
    ")\n",
    "# tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "# tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "\n",
    "def row_processor(row):\n",
    "    inputs = tokenizer(\n",
    "        row[\"text\"],\n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=256,\n",
    "    )\n",
    "    inputs['labels'] = inputs['input_ids'].copy()\n",
    "    return inputs\n",
    "\n",
    "# \n",
    "tokenized_dataset = {}\n",
    "for split in dataset_splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(row_processor, batched=True)\n",
    "\n",
    "# inspect the special token ids\n",
    "print(f\"{tokenizer.eos_token_id=}\")\n",
    "print(f\"{tokenizer.pad_token_id=}\")\n",
    "\n",
    "# inspect the columns in the tokenized dataset\n",
    "print(f\"{tokenized_dataset['train']=}\")\n",
    "print(f\"{tokenized_dataset['test']=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "gpt2_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=4,\n",
    "    id2label={ 0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\" },\n",
    "    label2id={ \"World\": 0, \"Sports\": 1, \"Business\": 2, \"Sci/Tech\": 3 },\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    #return_dict=True,\n",
    ")\n",
    "\n",
    "# ensure all the parameters of the base model are frozen.\n",
    "# see: https://huggingface.co/transformers/v4.2.2/training.html\n",
    "for param in gpt2_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# for name, module in gpt2_model.named_modules():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c647440-73bf-4ea4-beae-8564ec9a836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    fan_in_fan_out=True, # this is required for gpt2\n",
    "    modules_to_save=[\"h.10\"]\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(gpt2_model, lora_config)\n",
    "\n",
    "lora_model.print_trainable_parameters()\n",
    "# lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args=TrainingArguments(\n",
    "    output_dir=\"./data/news_gpt2_lora\",\n",
    "    # set the learning rate\n",
    "    learning_rate=0.0025,\n",
    "    # Set the per device train batch size and eval batch size\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    # evaluate and save the model after each epoch\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.02,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "print(f\"{training_args.device=}\")\n",
    "print(f\"{training_args.per_device_train_batch_size=}\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_pretrained_path=\"./gpt-lora\"\n",
    "lora_model.save_pretrained(lora_pretrained_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftModelForSequenceClassification\n",
    "\n",
    "model_to_merge = PeftModel.from_pretrained(\n",
    "    gpt2_model, \n",
    "    lora_pretrained_path,\n",
    ")\n",
    "\n",
    "merged_model = model_to_merge.merge_and_unload()\n",
    "merged_model.save_pretrained(lora_pretrained_path + \"-merged-model\", merged_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abb6505-4e1f-4763-8ea4-5505a346b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_merged_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    lora_pretrained_path + \"-merged-model\",\n",
    "    # num_labels=4,\n",
    "    # id2label={ 0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\" },\n",
    "    # label2id={ \"World\": 0, \"Sports\": 1, \"Business\": 2, \"Sci/Tech\": 3 },\n",
    "    # pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "# ensure all the parameters of the base model are frozen.\n",
    "# see: https://huggingface.co/transformers/v4.2.2/training.html\n",
    "for param in gpt2_merged_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3673f913-63ad-493d-bb3b-1e7cc1335734",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=gpt2_merged_model,\n",
    "    # args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
