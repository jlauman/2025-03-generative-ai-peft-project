{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -q \"scikit-learn~=1.6\" \\\n",
    "    \"datasets==3.3.2\" \"huggingface-hub==0.29.1\" \\\n",
    "    \"transformers==4.49.0\" \"evaluate==0.4.3\" \\\n",
    "    \"peft==0.14.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f296737-6b42-49fd-b150-fb2ac7cf5e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_id2label={0: 'World', 1: 'Sports', 2: 'Business', 3: 'Sci/Tech'}\n",
      "dataset_label2id={'World': 0, 'Sports': 1, 'Business': 2, 'Sci/Tech': 3}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_name = \"SetFit/ag_news\"\n",
    "dataset_splits = [\"train\", \"test\"]\n",
    "\n",
    "dataset_id2label={ 0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\" }\n",
    "dataset_label2id={v:k for k, v in dataset_id2label.items()}\n",
    "\n",
    "untuned_model_name = \"openai-community/gpt2\"\n",
    "\n",
    "lora_tuned_path=\"./gpt-lora-tuned\"\n",
    "\n",
    "print(f\"{dataset_id2label=}\")\n",
    "print(f\"{dataset_label2id=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7415d043-c071-471d-b87d-4789648926bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train = Dataset({\n",
      "    features: ['text', 'label', 'label_text'],\n",
      "    num_rows: 800\n",
      "})\n",
      "train[0] = {'text': 'Air France-KLM Sales Rise 6.4 on Passenger Increase (Update1) Air France-KLM Group, Europe #39;s biggest airline, said second-quarter sales grew 6.4 percent as passengers generated more revenue.', 'label': 2, 'label_text': 'Business'}\n",
      "--------------------\n",
      "test = Dataset({\n",
      "    features: ['text', 'label', 'label_text'],\n",
      "    num_rows: 200\n",
      "})\n",
      "test[0] = {'text': \"Alcoa Warns Earnings to Miss Forecasts (Reuters) Reuters - Alcoa Inc. , the world's largest\\\\aluminum producer, on Thursday warned that third-quarter\\\\results would fall far short of Wall Street expectations, hurt\\\\by plant shutdowns, restructuring costs and weakness in some\\\\markets.\", 'label': 2, 'label_text': 'Business'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# the ag_news dataset is split into 120k train rows and 7.6k test rows.\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# reduce record cound to fit in memory (and run faster)\n",
    "dataset[\"train\"] = dataset[\"train\"].shuffle(seed=7).select(range(800))\n",
    "dataset[\"test\"] = dataset[\"test\"].shuffle(seed=11).select(range(200))\n",
    "\n",
    "# view the dataset characteristics\n",
    "print(\"train =\", dataset[\"train\"])\n",
    "print(\"train[0] =\", dataset[\"train\"][0])\n",
    "print(\"--------------------\")\n",
    "print(\"test =\", dataset[\"test\"])\n",
    "print(\"test[0] =\", dataset[\"test\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc93268f-28e8-454a-9fa3-2f66a8054d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b05bc0e7ad4594b89d6c02c509380e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.eos_token_id=50256\n",
      "tokenizer.pad_token_id=50256\n",
      "tokenized_dataset['train']=Dataset({\n",
      "    features: ['text', 'label', 'label_text', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 800\n",
      "})\n",
      "tokenized_dataset['test']=Dataset({\n",
      "    features: ['text', 'label', 'label_text', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 200\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# https://huggingface.co/docs/transformers/en/model_doc/gpt2#usage-tips\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "# tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "\n",
    "# \n",
    "tokenized_dataset = {}\n",
    "for split in dataset_splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(\n",
    "        lambda row: tokenizer(row[\"text\"], truncation=True), batched=True\n",
    "    )\n",
    "\n",
    "# inspect the special token ids\n",
    "print(f\"{tokenizer.eos_token_id=}\")\n",
    "print(f\"{tokenizer.pad_token_id=}\")\n",
    "\n",
    "# inspect the columns in the tokenized dataset\n",
    "print(f\"{tokenized_dataset['train']=}\")\n",
    "print(f\"{tokenized_dataset['test']=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=4, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "untuned_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    untuned_model_name,\n",
    "    num_labels=len(dataset_id2label),\n",
    "    id2label=dataset_id2label,\n",
    "    label2id=dataset_id2label,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "# ensure all the parameters of the base model are frozen.\n",
    "# see: https://huggingface.co/transformers/v4.2.2/training.html\n",
    "for param in untuned_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(untuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.device=device(type='mps')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args=TrainingArguments(\n",
    "    output_dir=\"./data/gpt2-untuned\",\n",
    "    # set the learning rate\n",
    "    learning_rate=0.005,\n",
    "    # set the per device train batch size and eval batch size\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    # evaluate and save the model after each epoch\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "print(f\"{training_args.device=}\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=untuned_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "019b9f55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 5.677774429321289,\n",
       " 'eval_model_preparation_time': 0.0028,\n",
       " 'eval_accuracy': 0.265,\n",
       " 'eval_runtime': 3.012,\n",
       " 'eval_samples_per_second': 66.401,\n",
       " 'eval_steps_per_second': 16.6}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are interested in the accuracy of the untuned gpt2 model.\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c29fee2-875d-43fb-8f6a-38931a339f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a13f0242b524840ab6dd57544f227e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.eos_token_id=50256\n",
      "tokenizer.pad_token_id=50256\n",
      "tokenized_dataset['train']=Dataset({\n",
      "    features: ['text', 'label', 'label_text', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 800\n",
      "})\n",
      "tokenized_dataset['test']=Dataset({\n",
      "    features: ['text', 'label', 'label_text', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 200\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# https://huggingface.co/docs/transformers/en/model_doc/gpt2#usage-tips\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    untuned_model_name,\n",
    "    padding_side=\"right\"\n",
    ")\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "# tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "\n",
    "def row_processor(row):\n",
    "    inputs = tokenizer(\n",
    "        row[\"text\"],\n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=256,\n",
    "    )\n",
    "    inputs['labels'] = inputs['input_ids'].copy()\n",
    "    return inputs\n",
    "\n",
    "# \n",
    "tokenized_dataset = {}\n",
    "for split in dataset_splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(row_processor, batched=True)\n",
    "\n",
    "# inspect the special token ids\n",
    "print(f\"{tokenizer.eos_token_id=}\")\n",
    "print(f\"{tokenizer.pad_token_id=}\")\n",
    "\n",
    "# inspect the columns in the tokenized dataset\n",
    "print(f\"{tokenized_dataset['train']=}\")\n",
    "print(f\"{tokenized_dataset['test']=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "untuned_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    untuned_model_name,\n",
    "    num_labels=len(dataset_id2label),\n",
    "    id2label=dataset_id2label,\n",
    "    label2id=dataset_id2label,    \n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "# ensure all the parameters of the base model are frozen.\n",
    "# see: https://huggingface.co/transformers/v4.2.2/training.html\n",
    "for param in gpt2_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# for name, module in gpt2_model.named_modules():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c647440-73bf-4ea4-beae-8564ec9a836e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,172,288 || all params: 132,615,168 || trainable%: 6.1624\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    fan_in_fan_out=True, # this is required for gpt2\n",
    "    modules_to_save=[\"h.10\"]\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(untuned_model, lora_config)\n",
    "\n",
    "lora_model.print_trainable_parameters()\n",
    "# lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.device=device(type='mps')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args=TrainingArguments(\n",
    "    output_dir=\"./data/news_gpt2_lora\",\n",
    "    # set the learning rate\n",
    "    learning_rate=0.0025,\n",
    "    # Set the per device train batch size and eval batch size\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    # evaluate and save the model after each epoch\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.02,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "print(f\"{training_args.device=}\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 02:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.338342</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.407811</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.382158</td>\n",
       "      <td>0.895000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=0.4057020060221354, metrics={'train_runtime': 145.1669, 'train_samples_per_second': 16.533, 'train_steps_per_second': 1.033, 'total_flos': 343688085504000.0, 'train_loss': 0.4057020060221354, 'epoch': 3.0})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lora_model.save_pretrained(lora_tuned_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft_config.base_model_name_or_path='openai-community/gpt2'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftConfig, PeftModel\n",
    "#from peft import PeftModelForSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(lora_tuned_path)\n",
    "print(f\"{peft_config.base_model_name_or_path=}\")\n",
    "\n",
    "peft_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    num_labels=len(dataset_id2label),\n",
    "    id2label=dataset_id2label,\n",
    "    label2id=dataset_id2label,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "model_to_merge = PeftModel.from_pretrained(\n",
    "    peft_model, \n",
    "    lora_tuned_path,\n",
    ")\n",
    "\n",
    "merged_model = model_to_merge.merge_and_unload()\n",
    "merged_model.save_pretrained(lora_tuned_path + \"-merged-model\", merged_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4abb6505-4e1f-4763-8ea4-5505a346b732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=4, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "gpt2_lora_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    lora_tuned_path + \"-merged-model\",\n",
    "    num_labels=len(dataset_id2label),\n",
    "    id2label=dataset_id2label,\n",
    "    label2id=dataset_id2label,    \n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "# ensure all the parameters of the base model are frozen.\n",
    "# see: https://huggingface.co/transformers/v4.2.2/training.html\n",
    "for param in gpt2_merged_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3673f913-63ad-493d-bb3b-1e7cc1335734",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=gpt2_lora_model,\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.33834221959114075,\n",
       " 'eval_model_preparation_time': 0.0033,\n",
       " 'eval_accuracy': 0.875,\n",
       " 'eval_runtime': 4.6394,\n",
       " 'eval_samples_per_second': 43.109,\n",
       " 'eval_steps_per_second': 5.389}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
