{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q \"scikit-learn~=1.6\" \\\n",
    "    \"datasets==3.3.2\" \"huggingface-hub==0.29.1\" \\\n",
    "    \"transformers==4.49.0\" \"evaluate==0.4.3\" \\\n",
    "    \"peft==0.14.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f296737-6b42-49fd-b150-fb2ac7cf5e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_name = \"SetFit/ag_news\"\n",
    "dataset_splits = [\"train\", \"test\"]\n",
    "\n",
    "dataset_id2label={ 0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\" }\n",
    "dataset_label2id={v:k for k, v in dataset_id2label.items()}\n",
    "\n",
    "untuned_model_name = \"openai-community/gpt2\"\n",
    "\n",
    "lora_tuned_path=\"./data/gpt2-lora-tuned\"\n",
    "\n",
    "print(f\"{dataset_id2label=}\")\n",
    "print(f\"{dataset_label2id=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7415d043-c071-471d-b87d-4789648926bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# the ag_news dataset is split into 120k train rows and 7.6k test rows.\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# reduce record cound to fit in memory (and run faster)\n",
    "dataset[\"train\"] = dataset[\"train\"].shuffle(seed=7).select(range(2000))\n",
    "dataset[\"test\"] = dataset[\"test\"].shuffle(seed=11).select(range(200))\n",
    "\n",
    "# view the dataset characteristics\n",
    "print(\"train =\", dataset[\"train\"])\n",
    "print(\"train[0] =\", dataset[\"train\"][0])\n",
    "print(\"--------------------\")\n",
    "print(\"test =\", dataset[\"test\"])\n",
    "print(\"test[0] =\", dataset[\"test\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc93268f-28e8-454a-9fa3-2f66a8054d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# https://huggingface.co/docs/transformers/en/model_doc/gpt2#usage-tips\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    untuned_model_name,\n",
    "    padding_side = \"right\",\n",
    ")\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "# tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "\n",
    "def row_processor(row):\n",
    "    inputs = tokenizer(\n",
    "        row[\"text\"],\n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=256,\n",
    "    )\n",
    "    inputs['labels'] = inputs['input_ids'].copy()\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = {}\n",
    "for split in dataset_splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(row_processor, batched=True)\n",
    "\n",
    "# inspect the special token ids\n",
    "print(f\"{tokenizer.eos_token_id=}\")\n",
    "print(f\"{tokenizer.pad_token_id=}\")\n",
    "\n",
    "# inspect the columns in the tokenized dataset\n",
    "print(f\"{tokenized_dataset['train']=}\")\n",
    "print(f\"{tokenized_dataset['test']=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "untuned_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    untuned_model_name,\n",
    "    num_labels=len(dataset_id2label),\n",
    "    id2label=dataset_id2label,\n",
    "    label2id=dataset_id2label,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "# ensure all the parameters of the base model are frozen.\n",
    "# see: https://huggingface.co/transformers/v4.2.2/training.html\n",
    "for param in untuned_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(untuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args=TrainingArguments(\n",
    "    output_dir=\"./data/gpt2-untuned\",\n",
    "    # set the learning rate\n",
    "    learning_rate=0.005,\n",
    "    # set the per device train batch size and eval batch size\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    # evaluate and save the model after each epoch\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "print(f\"{training_args.device=}\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=untuned_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b9f55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we are interested in the accuracy of the untuned gpt2 model.\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c29fee2-875d-43fb-8f6a-38931a339f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# https://huggingface.co/docs/transformers/en/model_doc/gpt2#usage-tips\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    untuned_model_name,\n",
    "    padding_side=\"right\"\n",
    ")\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "\n",
    "def row_processor(row):\n",
    "    inputs = tokenizer(\n",
    "        row[\"text\"],\n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=256,\n",
    "    )\n",
    "    inputs['labels'] = inputs['input_ids'].copy()\n",
    "    return inputs\n",
    "\n",
    "# \n",
    "tokenized_dataset = {}\n",
    "for split in dataset_splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(row_processor, batched=True)\n",
    "\n",
    "# inspect the special token ids\n",
    "print(f\"{tokenizer.eos_token_id=}\")\n",
    "print(f\"{tokenizer.pad_token_id=}\")\n",
    "\n",
    "# inspect the columns in the tokenized dataset\n",
    "print(f\"{tokenized_dataset['train']=}\")\n",
    "print(f\"{tokenized_dataset['test']=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "untuned_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    untuned_model_name,\n",
    "    num_labels=len(dataset_id2label),\n",
    "    id2label=dataset_id2label,\n",
    "    label2id=dataset_id2label,    \n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "# ensure all the parameters of the base model are frozen.\n",
    "# see: https://huggingface.co/transformers/v4.2.2/training.html\n",
    "for param in untuned_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# for name, module in untuned_model.named_modules():\n",
    "#     print(name, type(module))\n",
    "\n",
    "# print(\"named_parameters...\")\n",
    "# for name, param in untuned_model.base_model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.requires_grad)\n",
    "\n",
    "print(untuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c647440-73bf-4ea4-beae-8564ec9a836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# these two steps need to be in the same cell because get_peft_model\n",
    "# will modify the untrained_model and target modules names will change.\n",
    "\n",
    "untuned_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    untuned_model_name,\n",
    "    num_labels=len(dataset_id2label),\n",
    "    id2label=dataset_id2label,\n",
    "    label2id=dataset_id2label,    \n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "# ensure all the parameters of the base model are frozen.\n",
    "# see: https://huggingface.co/transformers/v4.2.2/training.html\n",
    "for param in untuned_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# for name, module in untuned_model.named_modules():\n",
    "#     print(name, type(module))\n",
    "\n",
    "print(\"untuned_model...\")\n",
    "print(untuned_model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    fan_in_fan_out=True, # this is required for gpt2\n",
    "    target_modules=[\n",
    "        \"transformer.h.10.attn.c_proj\",\n",
    "        \"transformer.h.10.attn.c_proj\"\n",
    "        \"transformer.h.11.attn.c_proj\",\n",
    "        \"transformer.h.11.attn.c_proj\"\n",
    "    ],\n",
    "    modules_to_save=[\"score\"]\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(untuned_model, lora_config)\n",
    "\n",
    "print(\"--------------------\")\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "# for name, module in lora_model.named_modules():\n",
    "#     print(name, type(module))\n",
    "\n",
    "# for name, param in untuned_model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.requires_grad)\n",
    "\n",
    "print(\"lora_model...\")\n",
    "print(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args=TrainingArguments(\n",
    "    output_dir=\"./data/gpt2-lora-tuned\",\n",
    "    # set the learning rate\n",
    "    learning_rate=0.005,\n",
    "    # Set the per device train batch size and eval batch size\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    # evaluate and save the model after each epoch\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "print(f\"{training_args.device=}\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "training_args = None\n",
    "trainer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lora_model.save_pretrained(lora_tuned_path)\n",
    "lora_model = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftConfig, PeftModel\n",
    "from peft import PeftModelForSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(lora_tuned_path)\n",
    "print(f\"{peft_config.base_model_name_or_path=}\")\n",
    "print(peft_config)\n",
    "\n",
    "untuned_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    num_labels=len(dataset_id2label),\n",
    "    id2label=dataset_id2label,\n",
    "    label2id=dataset_id2label,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    return_dict=True,\n",
    ")\n",
    "\n",
    "lora_tuned_model = PeftModel.from_pretrained(\n",
    "    untuned_model, \n",
    "    lora_tuned_path,\n",
    ")\n",
    "\n",
    "lora_tuned_merged_model = lora_tuned_model.merge_and_unload()\n",
    "\n",
    "# ensure all the parameters of the base model are frozen.\n",
    "for param in lora_tuned_merged_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# merged_model.save_pretrained(lora_tuned_path + \"-merged-model\", merged_model)\n",
    "print(lora_tuned_merged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3673f913-63ad-493d-bb3b-1e7cc1335734",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_tuned_merged_model,\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.DataFrame(tokenized_dataset[\"test\"])\n",
    "df = df[[\"text\", \"label\"]]\n",
    "\n",
    "predictions = trainer.predict(tokenized_dataset[\"test\"])\n",
    "df[\"predicted_label\"] = np.argmax(predictions[0], axis=1)\n",
    "df[\"predicted_label\"] = df[\"predicted_label\"].map(lambda id: dataset_id2label[id]) \n",
    "df[\"label\"] = df[\"label\"].map(lambda id: dataset_id2label[id]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e292b6-790b-4d29-b272-b9f6c583cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"label\"] == df[\"predicted_label\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c643ad2-dc7d-46af-8e91-7334c26cdf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"label\"] != df[\"predicted_label\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6c6e65-718a-4807-8e35-f7bb9950ddb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
