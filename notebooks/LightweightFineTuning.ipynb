{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: I will use Low-Rank Adaptation (LoRA) for the parameter-efficient fine-tuning process in this project. LoRA will provide a basis for understanding alternate PEFT techniques.\n",
    "* Model: I selected openai-community/gpt2 as the model for this project because it is a smaller model (137 million parameters) and will serve as a good starting point for learning the PEFT process.\n",
    "* Evaluation approach: I will use the Hugging Face `Trainer` which provides a high-level interface for training and evaluation.\n",
    "* Fine-tuning dataset: The dataset I selected for this project is SetFit/ag_news which is labeled for classification of news summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure these libraries are installed.\n",
    "# this should be handled by the Python venv and requirements.txt file.\n",
    "! pip install -q \"scikit-learn~=1.6\" \\\n",
    "    \"datasets==3.3.2\" \"huggingface-hub==0.29.1\" \\\n",
    "    \"transformers==4.49.0\" \"evaluate==0.4.3\" \\\n",
    "    \"peft==0.14.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f296737-6b42-49fd-b150-fb2ac7cf5e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_id2label={0: 'World', 1: 'Sports', 2: 'Business', 3: 'Sci/Tech'}\n",
      "dataset_label2id={'World': 0, 'Sports': 1, 'Business': 2, 'Sci/Tech': 3}\n"
     ]
    }
   ],
   "source": [
    "# this project will reuse these definitions throughout the project.\n",
    "dataset_name = \"SetFit/ag_news\"\n",
    "dataset_splits = [\"train\", \"test\"]\n",
    "\n",
    "# labels in the dataset. see: https://huggingface.co/datasets/SetFit/ag_news\n",
    "dataset_id2label={ 0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\" }\n",
    "dataset_label2id={v:k for k, v in dataset_id2label.items()}\n",
    "\n",
    "untuned_model_name = \"openai-community/gpt2\"\n",
    "\n",
    "lora_tuned_path=\"./data/gpt2-lora-tuned\"\n",
    "\n",
    "print(f\"{dataset_id2label=}\")\n",
    "print(f\"{dataset_label2id=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7415d043-c071-471d-b87d-4789648926bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train = Dataset({\n",
      "    features: ['text', 'label', 'label_text'],\n",
      "    num_rows: 500\n",
      "})\n",
      "train[0] = {'text': 'Air France-KLM Sales Rise 6.4 on Passenger Increase (Update1) Air France-KLM Group, Europe #39;s biggest airline, said second-quarter sales grew 6.4 percent as passengers generated more revenue.', 'label': 2, 'label_text': 'Business'}\n",
      "--------------------\n",
      "test = Dataset({\n",
      "    features: ['text', 'label', 'label_text'],\n",
      "    num_rows: 100\n",
      "})\n",
      "test[0] = {'text': \"Alcoa Warns Earnings to Miss Forecasts (Reuters) Reuters - Alcoa Inc. , the world's largest\\\\aluminum producer, on Thursday warned that third-quarter\\\\results would fall far short of Wall Street expectations, hurt\\\\by plant shutdowns, restructuring costs and weakness in some\\\\markets.\", 'label': 2, 'label_text': 'Business'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# the ag_news dataset is split into 120k train rows and 7.6k test rows.\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# reduce record cound to fit in memory (and run faster).\n",
    "# the low record count will also test how well gpt2 does with classification\n",
    "# when the training data set is small compared to what is required for training\n",
    "# a model from scratch.\n",
    "dataset[\"train\"] = dataset[\"train\"].shuffle(seed=7).select(range(500))\n",
    "dataset[\"test\"] = dataset[\"test\"].shuffle(seed=11).select(range(100))\n",
    "\n",
    "# view the dataset characteristics\n",
    "print(\"train =\", dataset[\"train\"])\n",
    "print(\"train[0] =\", dataset[\"train\"][0])\n",
    "print(\"--------------------\")\n",
    "print(\"test =\", dataset[\"test\"])\n",
    "print(\"test[0] =\", dataset[\"test\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc93268f-28e8-454a-9fa3-2f66a8054d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.eos_token_id=50256\n",
      "tokenizer.pad_token_id=50257\n",
      "tokenized_dataset['train']=Dataset({\n",
      "    features: ['text', 'label', 'label_text', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 500\n",
      "})\n",
      "tokenized_dataset['test']=Dataset({\n",
      "    features: ['text', 'label', 'label_text', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 100\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# https://huggingface.co/docs/transformers/en/model_doc/gpt2#usage-tips\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    untuned_model_name,\n",
    "    padding_side = \"right\",\n",
    ")\n",
    "# information regarding gpt2 and padding in tokenizer\n",
    "# see: https://medium.com/@prashanth.ramanathan/fine-tuning-a-pre-trained-gpt-2-model-and-performing-inference-a-hands-on-guide-57c097a3b810\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "\n",
    "def row_processor(row):\n",
    "    inputs = tokenizer(\n",
    "        row[\"text\"],\n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=256,\n",
    "    )\n",
    "    inputs['labels'] = inputs['input_ids'].copy()\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = {}\n",
    "for split in dataset_splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(row_processor, batched=True)\n",
    "\n",
    "# inspect the special token ids\n",
    "print(f\"{tokenizer.eos_token_id=}\")\n",
    "print(f\"{tokenizer.pad_token_id=}\")\n",
    "\n",
    "# inspect the columns in the tokenized dataset\n",
    "print(f\"{tokenized_dataset['train']=}\")\n",
    "print(f\"{tokenized_dataset['test']=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=4, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "untuned_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    untuned_model_name,\n",
    "    num_labels=len(dataset_id2label),\n",
    "    id2label=dataset_id2label,\n",
    "    label2id=dataset_id2label,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "# ensure all the parameters of the base model are frozen\n",
    "# except for the score layer.\n",
    "# see: https://huggingface.co/transformers/v4.2.2/training.html\n",
    "for name, param in untuned_model.base_model.named_parameters():\n",
    "    if \"score\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# ensure the score layer outputs 4 values.\n",
    "print(untuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.device=device(type='mps')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args=TrainingArguments(\n",
    "    output_dir=\"./data/gpt2-untuned\",\n",
    "    # set the learning rate\n",
    "    learning_rate=0.005,\n",
    "    # set the per device train batch size and eval batch size\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    # evaluate and save the model after each epoch\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "print(f\"{training_args.device=}\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=untuned_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2129e136-b30e-469c-a58a-a8d6e99db095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 00:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.468647</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.479229</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.425354</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=96, training_loss=0.6579697529474894, metrics={'train_runtime': 46.3028, 'train_samples_per_second': 32.395, 'train_steps_per_second': 2.073, 'total_flos': 195976101888000.0, 'train_loss': 0.6579697529474894, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to do a training pass to set the weights on the score\n",
    "# layer. the goal is to compare the base gpt2 model against a PEFT\n",
    "# adapted gpt2 model, so we need have the score layer trained on\n",
    "# the base model.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "019b9f55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.42535400390625,\n",
       " 'eval_accuracy': 0.83,\n",
       " 'eval_runtime': 2.1149,\n",
       " 'eval_samples_per_second': 47.282,\n",
       " 'eval_steps_per_second': 3.31,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are interested in the accuracy of the untuned gpt2 model.\n",
    "eval_prediction = trainer.evaluate()\n",
    "untuned_prediction_accuracy = eval_prediction[\"eval_accuracy\"]\n",
    "eval_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c647440-73bf-4ea4-beae-8564ec9a836e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "untuned_model...\n",
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=4, bias=False)\n",
      ")\n",
      "--------------------\n",
      "trainable params: 592,896 || all params: 125,035,776 || trainable%: 0.4742\n",
      "lora_model...\n",
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2ForSequenceClassification(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 768)\n",
      "        (wpe): Embedding(1024, 768)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-2): 3 x GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): Conv1D(nf=2304, nx=768)\n",
      "              (c_proj): Conv1D(nf=768, nx=768)\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D(nf=3072, nx=768)\n",
      "              (c_proj): Conv1D(nf=768, nx=3072)\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): lora.Linear(\n",
      "                (base_layer): Conv1D(nf=2304, nx=768)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=2304, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (c_proj): lora.Linear(\n",
      "                (base_layer): Conv1D(nf=768, nx=768)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D(nf=3072, nx=768)\n",
      "              (c_proj): Conv1D(nf=768, nx=3072)\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): Conv1D(nf=2304, nx=768)\n",
      "              (c_proj): Conv1D(nf=768, nx=768)\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D(nf=3072, nx=768)\n",
      "              (c_proj): Conv1D(nf=768, nx=3072)\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): lora.Linear(\n",
      "                (base_layer): Conv1D(nf=2304, nx=768)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=2304, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (c_proj): lora.Linear(\n",
      "                (base_layer): Conv1D(nf=768, nx=768)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D(nf=3072, nx=768)\n",
      "              (c_proj): Conv1D(nf=768, nx=3072)\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6-11): 6 x GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): Conv1D(nf=2304, nx=768)\n",
      "              (c_proj): Conv1D(nf=768, nx=768)\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D(nf=3072, nx=768)\n",
      "              (c_proj): Conv1D(nf=768, nx=3072)\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (score): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=4, bias=False)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=4, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# these two steps need to be in the same cell because get_peft_model\n",
    "# will modify the untrained_model and target modules names will change.\n",
    "\n",
    "untuned_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    untuned_model_name,\n",
    "    num_labels=len(dataset_id2label),\n",
    "    id2label=dataset_id2label,\n",
    "    label2id=dataset_id2label,    \n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "# untuned_model.score.weight.data = score_weight_tensor\n",
    "# print(untuned_model.score.weight.clone())\n",
    "\n",
    "# ensure all the parameters of the base model are frozen\n",
    "# including the score layer.\n",
    "# see: https://huggingface.co/transformers/v4.2.2/training.html\n",
    "for param in untuned_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# for name, module in untuned_model.base_model.named_modules():\n",
    "#     print(name, type(module))\n",
    "\n",
    "print(\"untuned_model...\")\n",
    "print(untuned_model)\n",
    "\n",
    "# the following article was used as a source understanding how/why\n",
    "# rank (r) is set, and the understanding the relationship between\n",
    "# rank and alpha in LoRA.\n",
    "# https://medium.com/@fartypantsham/what-rank-r-and-alpha-to-use-in-lora-in-llm-1b4f025fd133\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    fan_in_fan_out=True, # this is required for gpt2\n",
    "    # from trail-and-error it appears that the attention layers in the gp2\n",
    "    # model are the ones that most improved accuracy from the PEFT.\n",
    "    target_modules=[\n",
    "        \"transformer.h.3.attn.c_attn\",\n",
    "        \"transformer.h.3.attn.c_proj\",\n",
    "        \"transformer.h.5.attn.c_attn\",\n",
    "        \"transformer.h.5.attn.c_proj\",\n",
    "        # \"transformer.h.7.attn.c_attn\",\n",
    "        # \"transformer.h.7.attn.c_proj\",\n",
    "        # \"transformer.h.10.attn.c_attn\",\n",
    "        # \"transformer.h.10.attn.c_proj\",\n",
    "        # \"transformer.h.11.attn.c_attn\",\n",
    "        # \"transformer.h.11.attn.c_proj\"\n",
    "    ],\n",
    "    modules_to_save=[\"score\"]\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(untuned_model, lora_config)\n",
    "\n",
    "print(\"--------------------\")\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "# for name, module in lora_model.named_modules():\n",
    "#     print(name, type(module))\n",
    "\n",
    "# for name, param in untuned_model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.requires_grad)\n",
    "\n",
    "print(\"lora_model...\")\n",
    "print(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.device=device(type='mps')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# these training arguments must be the same as the gpt2 \n",
    "# model training and evaluation step used to establish the\n",
    "# baseline accuracy.\n",
    "training_args=TrainingArguments(\n",
    "    output_dir=\"./data/gpt2-lora-tuned\",\n",
    "    # set the learning rate\n",
    "    learning_rate=0.005,\n",
    "    # Set the per device train batch size and eval batch size\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    # evaluate and save the model after each epoch\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "print(f\"{training_args.device=}\")\n",
    "\n",
    "# use the same tokenized_dataset as the untuned evaluation.\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 01:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.589060</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.348991</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.304491</td>\n",
       "      <td>0.910000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=96, training_loss=0.5302836100260416, metrics={'train_runtime': 74.5541, 'train_samples_per_second': 20.12, 'train_steps_per_second': 1.288, 'total_flos': 197342134272000.0, 'train_loss': 0.5302836100260416, 'epoch': 3.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the LoRA adapter\n",
    "lora_model.save_pretrained(lora_tuned_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft_config.base_model_name_or_path='openai-community/gpt2'\n",
      "LoraConfig(task_type='SEQ_CLS', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='openai-community/gpt2', revision=None, inference_mode=True, r=64, target_modules={'transformer.h.3.attn.c_attn', 'transformer.h.5.attn.c_proj', 'transformer.h.3.attn.c_proj', 'transformer.h.5.attn.c_attn'}, exclude_modules=None, lora_alpha=64, lora_dropout=0.05, fan_in_fan_out=True, bias='none', use_rslora=False, modules_to_save=['score', 'classifier', 'score'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=4, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftConfig, PeftModel\n",
    "from peft import PeftModelForSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(lora_tuned_path)\n",
    "print(f\"{peft_config.base_model_name_or_path=}\")\n",
    "print(peft_config)\n",
    "\n",
    "untuned_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    num_labels=len(dataset_id2label),\n",
    "    id2label=dataset_id2label,\n",
    "    label2id=dataset_id2label,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    return_dict=True,\n",
    ")\n",
    "\n",
    "lora_tuned_model = PeftModel.from_pretrained(\n",
    "    untuned_model, \n",
    "    lora_tuned_path,\n",
    ")\n",
    "\n",
    "lora_tuned_merged_model = lora_tuned_model.merge_and_unload()\n",
    "\n",
    "# ensure all the parameters of the base model are frozen.\n",
    "for param in lora_tuned_merged_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# merged_model.save_pretrained(lora_tuned_path + \"-merged-model\", merged_model)\n",
    "print(lora_tuned_merged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3673f913-63ad-493d-bb3b-1e7cc1335734",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_tuned_merged_model,\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3044907748699188,\n",
       " 'eval_model_preparation_time': 0.0009,\n",
       " 'eval_accuracy': 0.91,\n",
       " 'eval_runtime': 2.2741,\n",
       " 'eval_samples_per_second': 43.974,\n",
       " 'eval_steps_per_second': 5.717}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_prediction = trainer.evaluate()\n",
    "tuned_prediction_accuracy = eval_prediction[\"eval_accuracy\"]\n",
    "eval_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a728abe-ba3d-4845-95ae-4b12fe68b9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "untuned_prediction_accuracy=0.83\n",
      "tuned_prediction_accuracy=0.91\n",
      "9.64% accuracy improvement\n",
      "trainable params: 592,896 || all params: 125,035,776 || trainable%: 0.4742\n"
     ]
    }
   ],
   "source": [
    "print(f\"{untuned_prediction_accuracy=}\")\n",
    "print(f\"{tuned_prediction_accuracy=}\")\n",
    "print(f\"{(((tuned_prediction_accuracy - untuned_prediction_accuracy) / untuned_prediction_accuracy) * 100):.2f}% accuracy improvement\")\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.DataFrame(tokenized_dataset[\"test\"])\n",
    "df = df[[\"text\", \"label\"]]\n",
    "\n",
    "predictions = trainer.predict(tokenized_dataset[\"test\"])\n",
    "df[\"predicted_label\"] = np.argmax(predictions[0], axis=1)\n",
    "df[\"predicted_label\"] = df[\"predicted_label\"].map(lambda id: dataset_id2label[id]) \n",
    "df[\"label\"] = df[\"label\"].map(lambda id: dataset_id2label[id]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52e292b6-790b-4d29-b272-b9f6c583cd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alcoa Warns Earnings to Miss Forecasts (Reuters) Reuters - Alcoa Inc. , the world's largest\\aluminum producer, on Thursday warned that third-quarter\\results would fall far short of Wall Street expectations, hurt\\by plant shutdowns, restructuring costs and weakness in some\\markets.</td>\n",
       "      <td>Business</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Panama Assures Rumsfeld on Canal Security  PANAMA CITY, Panama (Reuters) - Panama's security chief  told Defense Secretary Donald Rumsfeld on Saturday the Central  American nation was working to prevent any terror attack that  might close the Panama Canal.</td>\n",
       "      <td>World</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prosecutor brings charges against former neighbor in NBA brawl PONTIAC, Mich. The man accused of starting the brawl at a Detroit Pistons game last month is no stranger to the man who #39;s filing the charges against him.</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Palace Bans Two Fans The Palace in Auburn Hills bans two men from events for their involvement in last month's brawl between the Pistons and Indian Pacers.</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New MSN Search May Be a Google Killer! New MSN Search May Be a Google Killer!\\\\The Second Look at MSN's Search technology is available for public beta testing. I've given it a spin myself and must say that I'm impressed. Although they have no ads on the SERP's of the preview site, I'm sure they will load it ...</td>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jeanne Heads for Bahamas After Killing 3 SAMANA, Dominican Republic - Threatening to regain hurricane strength, Tropical Storm Jeanne headed for the Bahamas on a track for the southeastern United States after killing three people and causing extensive damage in the Caribbean.    The storm forced the evacuation of thousands on Thursday as it slammed into the Dominican Republic after punishing Puerto Rico with flash floods and deadly winds...</td>\n",
       "      <td>World</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Coke opens its coolers to rival products Coca Cola is to allow other companies #39; products in its shop coolers for the first time. It has agreed the move in a deal with the European Commission to settle a five year competition case.</td>\n",
       "      <td>Business</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ECB  quot;consensus quot; kept rates steady today European Central Bank president Jean-Claude Trichet has said that today #39;s decision to leave euro interest rates unchanged reflected a broad consensus on the governing council of the bank.</td>\n",
       "      <td>Business</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CARE Official Kidnapped in Baghdad Margaret Hassan, said to be a British-born Iraqi national, the director of CARE International #39;s operation in Iraq is seen in this image made from video footage made on May 20, 2003.</td>\n",
       "      <td>World</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>All the world #39;s a web page as the Bard goes online The earliest editions of Shakespeare #39;s plays provide a fascinating insight into how the playwright reworked his masterpieces over time, but until now, due to their age and</td>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                           text  \\\n",
       "0                                                                                                                                                                     Alcoa Warns Earnings to Miss Forecasts (Reuters) Reuters - Alcoa Inc. , the world's largest\\aluminum producer, on Thursday warned that third-quarter\\results would fall far short of Wall Street expectations, hurt\\by plant shutdowns, restructuring costs and weakness in some\\markets.   \n",
       "1                                                                                                                                                                                              Panama Assures Rumsfeld on Canal Security  PANAMA CITY, Panama (Reuters) - Panama's security chief  told Defense Secretary Donald Rumsfeld on Saturday the Central  American nation was working to prevent any terror attack that  might close the Panama Canal.   \n",
       "2                                                                                                                                                                                                                                  Prosecutor brings charges against former neighbor in NBA brawl PONTIAC, Mich. The man accused of starting the brawl at a Detroit Pistons game last month is no stranger to the man who #39;s filing the charges against him.   \n",
       "3                                                                                                                                                                                                                                                                                                   Palace Bans Two Fans The Palace in Auburn Hills bans two men from events for their involvement in last month's brawl between the Pistons and Indian Pacers.   \n",
       "4                                                                                                                                      New MSN Search May Be a Google Killer! New MSN Search May Be a Google Killer!\\\\The Second Look at MSN's Search technology is available for public beta testing. I've given it a spin myself and must say that I'm impressed. Although they have no ads on the SERP's of the preview site, I'm sure they will load it ...   \n",
       "5  Jeanne Heads for Bahamas After Killing 3 SAMANA, Dominican Republic - Threatening to regain hurricane strength, Tropical Storm Jeanne headed for the Bahamas on a track for the southeastern United States after killing three people and causing extensive damage in the Caribbean.    The storm forced the evacuation of thousands on Thursday as it slammed into the Dominican Republic after punishing Puerto Rico with flash floods and deadly winds...   \n",
       "6                                                                                                                                                                                                                    Coke opens its coolers to rival products Coca Cola is to allow other companies #39; products in its shop coolers for the first time. It has agreed the move in a deal with the European Commission to settle a five year competition case.   \n",
       "7                                                                                                                                                                                                             ECB  quot;consensus quot; kept rates steady today European Central Bank president Jean-Claude Trichet has said that today #39;s decision to leave euro interest rates unchanged reflected a broad consensus on the governing council of the bank.   \n",
       "8                                                                                                                                                                                                                                  CARE Official Kidnapped in Baghdad Margaret Hassan, said to be a British-born Iraqi national, the director of CARE International #39;s operation in Iraq is seen in this image made from video footage made on May 20, 2003.   \n",
       "9                                                                                                                                                                                                                       All the world #39;s a web page as the Bard goes online The earliest editions of Shakespeare #39;s plays provide a fascinating insight into how the playwright reworked his masterpieces over time, but until now, due to their age and    \n",
       "\n",
       "      label predicted_label  \n",
       "0  Business        Business  \n",
       "1     World           World  \n",
       "2    Sports          Sports  \n",
       "3    Sports          Sports  \n",
       "4  Sci/Tech        Sci/Tech  \n",
       "5     World           World  \n",
       "6  Business        Business  \n",
       "7  Business        Business  \n",
       "8     World           World  \n",
       "9  Sci/Tech        Sci/Tech  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"label\"] == df[\"predicted_label\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c643ad2-dc7d-46af-8e91-7334c26cdf06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BHP Billiton, Alcoa sell Integris Metals for 359 million pounds (AFP) AFP - Anglo-Australian mining giant BHP Billiton and Alcoa, the world's largest aluminium producer, have agreed to sell their metal services joint venture Integris Metals for 660 million dollars (359 million pounds) including debt, a joint statement said.</td>\n",
       "      <td>World</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Ford underlines committed to motorsport. Despite confirming the successful sale of both Jaguar Racing and its Cosworth engine company to new owners, Ford Motor Company has stressed that it remains committed to supporting motorsport at all levels.</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Martha Stewart reports to jail to begin sentence the time she had to report to the country #39;s oldest federal prison for women. service of her sentence, quot; a Federal Bureau of Prisons statement said.</td>\n",
       "      <td>Business</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Sales of industrial robots surging: UN report Geneva - Worldwide sales of industrial robots surged to record levels in the first half of 2004 after equipment prices fell while labour costs grew, the United Nations Economic Commission for Europe said in a report to be released today.</td>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>EU Head Office Trims 2005 Growth Forecast (AP) AP - The European Union's head office issued a bleak economic report Tuesday, warning that the sharp rise in oil prices will \"take its toll\" on economic growth next year while the euro's renewed climb could threaten crucial exports.</td>\n",
       "      <td>World</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Report: EADS Could Link With Thales The French government is considering a linkup of European Aeronautic Defence  amp; Space Co. with Thales SA to create an aerospace giant, the financial daily Les Echos reported Friday.</td>\n",
       "      <td>Business</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>La. Seeks New Bridge, Elevated Highway If you think oil is expensive now, just imagine if Hurricane Ivan had swung west and come ashore at this bustling oil and gas port at the southernmost point of Louisiana.</td>\n",
       "      <td>Business</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>GAME UNDER FIRE Attacking police officers, racial slurs, bloody beatings of innocent bystanders ... is it really just a game? In four and a half minutes, 14-year-old Ryan Mason ran over a police officer, stole his gun and shot and killed three innocent bystanders.</td>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ScanSoft to acquire 3 software firms ScanSoft Inc. said it plans three acquisitions. The company will acquire Phonetic Systems Ltd., a provider of automated directory assistance and voice-based programs, for \\$35 million in cash, and an additional consideration of up to \\$35 million, based on the achievement of performance targets and the potential vesting of a warrant to buy 750,000 common shares. ART Advanced Recognition Technologies ...</td>\n",
       "      <td>Business</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                           text  \\\n",
       "18                                                                                                                        BHP Billiton, Alcoa sell Integris Metals for 359 million pounds (AFP) AFP - Anglo-Australian mining giant BHP Billiton and Alcoa, the world's largest aluminium producer, have agreed to sell their metal services joint venture Integris Metals for 660 million dollars (359 million pounds) including debt, a joint statement said.   \n",
       "24                                                                                                                                                                                                       Ford underlines committed to motorsport. Despite confirming the successful sale of both Jaguar Racing and its Cosworth engine company to new owners, Ford Motor Company has stressed that it remains committed to supporting motorsport at all levels.   \n",
       "30                                                                                                                                                                                                                                                 Martha Stewart reports to jail to begin sentence the time she had to report to the country #39;s oldest federal prison for women. service of her sentence, quot; a Federal Bureau of Prisons statement said.   \n",
       "33                                                                                                                                                                  Sales of industrial robots surging: UN report Geneva - Worldwide sales of industrial robots surged to record levels in the first half of 2004 after equipment prices fell while labour costs grew, the United Nations Economic Commission for Europe said in a report to be released today.   \n",
       "58                                                                                                                                                                      EU Head Office Trims 2005 Growth Forecast (AP) AP - The European Union's head office issued a bleak economic report Tuesday, warning that the sharp rise in oil prices will \"take its toll\" on economic growth next year while the euro's renewed climb could threaten crucial exports.   \n",
       "62                                                                                                                                                                                                                                 Report: EADS Could Link With Thales The French government is considering a linkup of European Aeronautic Defence  amp; Space Co. with Thales SA to create an aerospace giant, the financial daily Les Echos reported Friday.   \n",
       "75                                                                                                                                                                                                                                            La. Seeks New Bridge, Elevated Highway If you think oil is expensive now, just imagine if Hurricane Ivan had swung west and come ashore at this bustling oil and gas port at the southernmost point of Louisiana.   \n",
       "81                                                                                                                                                                                     GAME UNDER FIRE Attacking police officers, racial slurs, bloody beatings of innocent bystanders ... is it really just a game? In four and a half minutes, 14-year-old Ryan Mason ran over a police officer, stole his gun and shot and killed three innocent bystanders.   \n",
       "95  ScanSoft to acquire 3 software firms ScanSoft Inc. said it plans three acquisitions. The company will acquire Phonetic Systems Ltd., a provider of automated directory assistance and voice-based programs, for \\$35 million in cash, and an additional consideration of up to \\$35 million, based on the achievement of performance targets and the potential vesting of a warrant to buy 750,000 common shares. ART Advanced Recognition Technologies ...   \n",
       "\n",
       "       label predicted_label  \n",
       "18     World        Business  \n",
       "24    Sports        Business  \n",
       "30  Business           World  \n",
       "33  Sci/Tech        Business  \n",
       "58     World        Business  \n",
       "62  Business        Sci/Tech  \n",
       "75  Business           World  \n",
       "81  Sci/Tech           World  \n",
       "95  Business        Sci/Tech  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"label\"] != df[\"predicted_label\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6c6e65-718a-4807-8e35-f7bb9950ddb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
